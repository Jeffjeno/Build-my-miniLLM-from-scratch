# Build-my-miniLLM-from-scratch
The repository will include a complete technology stack in the construction of LLM from stratch,from the dataset construction ,model training to evaluation.
# 构建LLM推理增强微调pipeline

1. 数据准备阶段(2 days)

高质量的数据是提升模型推理能力的基础。在本阶段，我们需要选择和构建适合复杂推理任务的数据集，并对数据进行格式转换、思维链（Chain-of-Thought, CoT）标注和增强。
	•	数据集选择：优先选择包含复杂推理过程或标注有思维链的数据集。例如：
	•	GSM8K（Grade School Math 8K）：8.5K个人工撰写的小学数学文字题，通常需要多步计算推理才能解答 ￼。据研究，即使是大型Transformer模型在该数据集上的直接表现也不高，需要借助推理过程提示才能取得突破 ￼。
	•	AQUA-RAT（Algebra Question Answering with Rationales）：一个包含约10万道代数文字题的大型数据集，每个问题的解答都配有逐步的自然语言推理过程（rationale） ￼。这些逐步推理步骤为模型学习提供了范例，但需要注意其中的推理可能有噪声，需要清洗或校验。
	•	StrategyQA：一个是非问答数据集，问题需要隐式多步推理才能回答是或否 ￼。该数据集的问题通过众包方式收集，强调创造性和多样性，需要模型进行隐含的多跳推理 ￼。
	•	此外，还有其他推理数据集可供参考，如Math Word Problems（数学文字题，如MAWPS）、Commonsense QA（常识推理问答）、**Big-Bench Hard (BBH)**等，可根据需求组合使用。
	•	格式转换：将不同来源的数据转换为统一的问答+推理链格式。例如可以采用"问题：{Question}\n思考：{Chain-of-Thought}\n答案：{Answer}"的prompt模板。确保每条数据都包含：用户问题、模型的思维过程（如果有）以及最终答案。对不包含显式思维过程的数据，可以在答案前加上诸如“让我们一步步思考”的提示，引导模型产生推理过程。
	•	思维链标注（CoT Annotation）：如果数据集没有现成的推理过程，需要人工或借助教师模型来生成思维链标注。人工标注时，可请领域专家为部分问题撰写详细的解题步骤。大模型教师（如GPT-4）则可以用于批量生成推理过程：例如，对每个问题进行few-shot提示，要求模型给出逐步推理。研究表明，让模型在回答前生成连贯的思维链有助于提升复杂推理题的准确性 ￼（在MultiArith和GSM8K上引入CoT提示使性能提高了数倍 ￼）。像WizardMath等项目就借助GPT-4为数学题补充了解题步骤数据 ￼。
	•	数据增强方法：可以采用多种数据增强策略来丰富推理数据：
	•	多样化思维链：利用自洽性 (Self-Consistency) 采样方法，针对同一问题让教师模型生成多个不同的思维链和答案，过滤出正确或高质量的推理过程 ￼。这样可以为每个问题准备多种解题思路，增强学生模型的鲁棒性。
	•	问题改写与生成：对现有问题进行改写或变形（如改变数字、语境等）以生成新问题，同时使用已有思维链模板自动构造对应的解题过程。还可利用高性能模型通过逆向思维生成新题目：先有解答过程，再反推出问题，从而扩充训练集。
	•	跨数据集融合：将不同数据集的样本混合，例如将数学推理和常识推理的数据放在一起训练，使模型学习到不同领域的推理范式。同时注意平衡各数据源，以避免模型偏向某一类问题。

准备数据时要注意质量控制。对于自动生成的思维链，需要人工抽查正确性，或训练一个初步验证模型来筛除明显错误的推理。高质量、多样化且格式统一的训练数据将为后续微调奠定坚实基础。

2. 模型预处理与初始化(2days)

有了数据之后，我们选择合适的基座模型并进行初始化配置。模型选择应考虑参数规模、基础能力以及是否易于微调。常用的开源基座模型包括：
	•	LLaMA系列（Meta出品）：如LLaMA、LLaMA2等，提供7B到70B不同参数规模的英文基座模型，性能强劲。LLaMA2-70B在许多基准上表现优异，但也需要更高的算力。较小的LLaMA-13B或7B则适合资源有限的场景。
	•	通义千问 (Qwen) 系列（阿里巴巴出品）：如Qwen-7B，据报告在2.2万亿tokens上预训练，架构与LLaMA类似。Qwen-7B-Chat进一步对齐了人类指令，支持中英双语，对中文支持较好。
	•	Mistral 7B（Mistral AI出品）：一个7.3B参数的模型，在效率和性能上经过精心设计。据官方称，Mistral-7B在许多评测上超过了LLaMA2-13B的表现。它使用了更长上下文和架构改进，并以Apache2协议开源，适合商用项目。

选择模型后，我们需要进行模型结构初始化和配置：
	•	加载预训练权重：使用深度学习框架（如🤗 Transformers）加载选定模型的预训练权重和相应的Tokenizer。在初始化时确保不启用梯度（只读），以便后续微调时能冻结必要权重。
	•	参数高效微调 (PEFT) 配置：对于大模型，全参数微调成本高昂。常用PEFT方法有:
	•	LoRA（Low-Rank Adaptation）：冻结原模型权重，在每层插入可训练的低秩矩阵，只增加极少参数。例如设定秩$r=8$或$16$，可以大幅减少训练需要更新的参数量。LoRA方法简单高效，不改变推理延迟，已被广泛用于大模型领域。
	•	QLoRA（Quantized LoRA）：这是LoRA的改进版，结合了4-bit量化以进一步降低显存占用。QLoRA流程是将基座模型权重量化为4位表示（如NF4量化），同时仍然训练LoRA低秩权重。据报道，QLoRA使得在单张48GB GPU上微调65B模型成为可能。配置QLoRA时，一般采用如bnb（bitsandbytes）库的支持，并调节优化器以适应低精度。
	•	微调配置时需要选择合适的LoRA秩rank、缩放系数alpha等超参，以及决定对哪些层应用LoRA（通常在Transformer的注意力投影和FFN层应用）。
	•	Tokenizer处理：确保使用与基座模型相匹配的Tokenizer，并根据训练数据需要添加特殊标记。例如如果我们希望模型输出步骤标记“思考：”或特殊符号“####”等，可能需要将其加入词表。若使用了开源对话模板（如LLaMA的、标记等），训练数据也应包含相应标记以指示对话轮次或格式。Tokenizer初始化完成后，对训练集做一次转换以确认没有未知标记。
	•	Prompt模板构建：设计模型微调时使用的输入格式模板。不同模型可能有自带的prompt格式，例如LLaMA使用[INST] ... [/INST]标签表示指令开始和结束；而ChatGPT风格则有“系统/用户/助手”角色。我们可以制定统一的模板，比如：

用户：{问题}
助理：让我们一步步思考。\n{思维链}\n答案：{最终答案}

模板应清晰区分问题、思考过程和答案部分，这样在训练时模型能学习在回答时先输出思维过程再给答案的格式。如果采用对话形式，也可以用特殊token或换行来分隔。初始化时将该模板融入数据流水线，使后续训练一致地遵循这一格式。

完成以上步骤后，模型就准备好进入微调流程。通过挑选合适的预训练基座和配置PEFT，我们在兼顾性能和算力的情况下，搭建了一个可用于推理增强微调的模型基础。

3. 推理增强方法集成(1 weeks)

在模型微调过程中，我们希望集成多种推理增强方法，使模型不仅能模仿思维链，还能通过蒸馏、强化学习、自检等手段进一步提升推理能力。下面介绍当前主流的推理增强技术，以及如何在我们的Pipeline中结合它们。

(1) CoT模仿学习（监督微调思维链）

首先是思维链模仿学习，即通过监督微调让模型学会输出人类或教师模型的推理过程。这需要我们在训练数据中提供高质量的“问题->思维链->答案”示例，对模型进行有监督训练 (SFT)。研究已证明，加入显式的思维链作为训练信号可以明显提升模型在算术、常识推理任务上的准确率 ￼。例如，使用链式思维提示后，PaLM模型在数学问题上的性能提升了数倍 ￼。

在Pipeline中，实现CoT模仿学习很直接：
	•	使用准备好的带思维过程标注的数据集，采用交叉熵损失，让模型学习在回答前生成正确的推理步骤。
	•	为了让模型真正内化这种推理模式，可以在训练开始时使用较低的温度或teacher forcing，使其严格按照示例输出步骤。一段时间后再逐渐解除，鼓励模型自主地产生合理的推理步骤。
	•	训练过程中监控模型生成思维链的质量（例如在验证集上检查推理步骤的正确性）。如果发现模型倾向于忽略思维链直接给答案，可尝试加强prompt或增加思维链部分loss权重。

通过CoT监督微调，模型将习得“先想后答”的习惯，在复杂问题上能够逐步推演。这为后续的优化打下基础，也使模型初步具备了解释其答案的能力。

(2) 蒸馏（教师模型指导）

在许多情况下，小模型可以从更强大的教师模型那里受益，通过知识蒸馏来提升性能。具体而言，我们可以：
	•	利用性能卓越的教师LLM（如GPT-4、PaLM 2等）为训练数据中的每个问题生成高质量的思维链和答案。这些教师输出可以视为额外的训练示例，加入对学生模型的微调。
	•	如果有资源，可以构建交叉蒸馏流程：让教师模型针对同一问题生成多样化的思维过程和答案，然后挑选最佳的几个供学生学习。这类似于数据增强，也是一种知识综合。
	•	蒸馏过程还可以应用于最终答案层面，例如通过教师模型对学生模型的输出打分作为奖励信号，指导学生调优。但这种方式更属于RLHF范畴。

需要注意教师数据质量控制。虽说强模型输出往往正确，但也可能出现繁琐冗长的推理或不必要步骤，需要清洗。不少开源项目已成功应用蒸馏：例如WizardLM系列用GPT-4生成大量指令回答数据帮助小模型对齐；在数学领域，研究者也用GPT-4补充证明题的数据集链路以提升小模型的数学推理 ￼。通过蒸馏，学生模型能模仿教师的推理风格和技巧，在有限参数下逼近教师模型的表现。

(3) 强化学习范式微调（RLHF / GRPO / DPO）

单纯的监督学习难以覆盖人类偏好和复杂推理的方方面面，因此业界引入强化学习微调范式，让模型通过与环境（或人类偏好）的交互进一步优化输出。这里包含几种主要方法：
	•	RLHF（Reinforcement Learning from Human Feedback）：即从人类反馈中进行强化学习 ￼。RLHF典型地包含三个阶段：先经过SFT得到一个初步模型，然后收集人类偏好数据训练一个奖励模型（Reward Model），最后用策略优化算法（如PPO）让模型优化其输出以获得更高的奖励分数。在InstructGPT中，OpenAI成功通过人类偏好强化学习，让GPT-3模型能够更好地遵循指令。RLHF能够直接以人类评价为优化目标，例如让模型生成更有帮助或更真实的回答。不过RLHF训练不稳定之处在于，PPO等算法需要仔细平衡探索和约束（通常加入KL惩罚项以防止模型偏离初始分布），训练过程中也需要大量高质量的人类反馈数据支持。
	•	GRPO（Group Robust/Relative Preference Optimization）：这是近期提出的一类改进算法，旨在让强化学习更高效或更鲁棒。GRPO建立在PPO基础之上，使RLHF训练更精简快速，尤其适用于复杂推理任务。一种思路是直接使用偏好数据优化，无需额外训练一个奖励模型，从而减少内存和计算开销。例如DeepSeek团队在数学推理任务中引入了GRPO方法，用偏好数据直接更新策略，无需显式的奖励网络，从而降低了训练难度和内存消耗。据最新研究，GRPO还可以扩展为“群体鲁棒”优化，即针对不同来源的偏好数据（不同人群或难度）进行加权优化，最大化最差情况下的表现。在Pipeline中，如果采用GRPO，我们可以省去训练Reward Model的步骤，直接利用排序好的偏好对（preferred vs. dispreferred outputs）来优化模型，使其更符合理想行为。
	•	DPO（Direct Preference Optimization）：DPO是2023年提出的另一种无需强化学习的偏好微调方法。它将偏好学习问题直接转化为一个简单的有监督目标：给定一对模型输出（一个好一个差），提高模型对较好输出的得分即可。DPO使用一个对比分值的logistic损失来训练模型，使之在不需要PPO的情况下达到类似效果。据报道，DPO算法稳定、高效，可以取得与PPO相当的效果，同时避免了复杂的策略梯度计算。在实践中，我们需要有一批成对比较的数据（通常来源于人类偏好标注或人工合成），然后按照DPO公式更新模型参数。DPO的实现相对直接，Hugging Face的TRL库已经支持这种偏好微调方式。

在Pipeline中，可以根据项目需要选择上述一种或组合多种策略进行偏好对齐训练。例如，先用DPO在已有偏好数据上快速调整模型，然后若有更高要求再通过RLHF + PPO进一步提升。此外，要特别注意安全性和价值观对齐：偏好数据应尽可能多样且代表真实用户需求，训练时也要防止模型过度迎合某些偏好而失去一般能力。

(4) Verifier验证模型训练与推理过滤

引入验证者（Verifier）模型是一种提高推理正确率的实用方法。Verifier可以是一个单独训练的模型，用于在推理链生成后判断结果的正确性，或者在生成过程中为模型提供反馈。它的作用类似于裁判或质量控制：
	•	在训练阶段，我们可以用部分数据训练一个Verifier，让它输入“问题+模型解答（含推理过程）”，输出一个评价（如正确/错误概率）。这个Verifier本质上就是一个奖励模型或评分模型，和RLHF中的Reward Model类似。
	•	训练Verifier的方法包括：利用人类标注的对错信息进行监督训练，或让Verifier学习预测最终答案与标准答案是否匹配。对数学或代码等可自动判分的任务，也可以生成大量样本直接训练Verifier去模仿判分函数。
	•	Verifier模型不需要太大，例如可以用8B左右的模型专门训练成判别器。在多模态推理中也有类似思路，通过验证器保证每步推理既符合理性又符合视觉/文本证据。

在推理阶段，我们可以通过Verifier进行结果过滤和自我纠错：
	•	筛选：对于模型生成的多个候选解答，用Verifier评估每一个的正确性得分，只保留得分最高或高于阈值的答案作为最终输出。这种方式可以有效剔除一些明显错误的答案。例如，针对数学问题让生成模型给出答案后，用Verifier判定是否正确；如果不正确，可以弃用该解答并让生成模型再尝试别的方法。
	•	自我验证循环：模型先给出一个初步解答，然后Verifier判断其是否可能有误。如果Verifier认为有误，可以将这个反馈（例如错误类型或简单的“请检查你的推理”提示）反馈给生成模型，让其基于此进行改进。这种机制类似于模型自我反思或自纠。近期研究提出，小模型本身也具备一定自我校正能力，当配以强大的Verifier时效果更好 ￼。比如，让GPT-4充当Verifier帮助一个13B模型检查推理，就能显著提高后者的解题正确率 ￼。

需要注意Verifier本身的可靠性。如果Verifier过弱，可能错误地放过不良答案或错杀正确答案。因此，有时也会采用强模型作为验证（如GPT-4评估GPT-3.5的输出）。整体来说，在Pipeline中加入Verifier模块，相当于多了一层把关，能提升模型推理结果的可靠度和准确性。

(5) “思维树”搜索与自洽性（Tree-of-Thoughts / Self-Consistency）

除了让模型一次性地生成单一路径的推理过程，我们还可以让模型探索多个推理路径，从中筛选最佳答案。两种典型策略是思维链自洽性和思维树搜索：
	•	自洽性 (Self-Consistency)：这是一种针对Chain-of-Thought的改进解码策略 ￼。具体做法是：在回答一个问题时，用稍高的随机性（如temperature）让模型生成多条不同的思维链和相应答案，然后通过多数投票（或最高置信）选出最终答案。假设模型的思维过程每次都有一定随机性，那么多次独立推理后，正确的答案往往会在不同推理路径下重复出现，而错误的随机路径则各异。通过投票可以消除大部分偶然错误，大幅提高最终准确率。研究表明，自洽解码能显著提升模型在各种复杂推理任务（算术、常识问答等）上的表现 ￼。在实践中，我们可以设定生成5到10个链，然后统计答案频次；如遇平票也可以让模型再对比选一个最合理的。自洽策略不用改变模型参数，仅需在推理时多采样几次，属于推理阶段增强。
	•	思维树 (Tree-of-Thoughts, ToT)：这是最近提出的一种通用推理框架，允许模型在解题时进行类似搜索树的多步探索。如图所示，传统方法要么直接从输入得到单一路径输出 (a)，要么生成一条链式思维过程 (b)，或是生成多条独立链并以多数票选结果 (c)。而思维树方法 (d) 则让模型将问题拆解成一系列决策点：每一步生成多个候选想法 (thought)，形成树状分支结构；然后模型对这些中间想法进行评估（例如标记为“有潜力”或“行不通”），剪除不良分支，保留有前景的分支继续展开。这种方法结合了模型内评估和系统atic搜索（可采用宽度优先BFS或深度优先DFS等），允许模型在必要时回溯并尝试不同路径，从而更像人类“尝试—纠正”式地解决难题。Tree-of-Thoughts在需要规划、多步骤试错的任务上表现出色。例如在24点游戏等复杂搜索任务中，GPT-4通过简单链式思维只能解出4%的题，而引入ToT框架后成功率飙升到74%。在Pipeline实现上，ToT需要我们编写一个搜索控制器，与模型交互生成和评估想法——这相当于在推理阶段用算法增强模型能力。为此，我们可以：定义评估标准prompt，让模型对部分解答给出“可行/不确定/不可能”等判断；利用该判断选择下一个展开的节点，直到找到解。虽然实现较复杂，但ToT代表了推理流程优化的前沿方向，使LLM在推理时具备类似AI搜索的能力。

(6) 工具辅助推理（Tool-augmented Reasoning）

人类在推理时常借助外部工具（如纸笔计算、资料检索）。同样，让LLM学会调用工具也能增强其推理能力。典型的方法是ReAct框架（Reasoning + Acting）：
	•	在ReAct中，模型的输出被设计为交替的**“思维…>>行动”**的格式。例如，模型先输出一段思考“我们需要查找某某信息”，接着输出一个特殊token表示调用行动（如[查找])，然后开发者接收这个动作并调用相应API（比如搜索引擎）获取结果，再将结果反馈给模型，模型继续下一步推理。如此循环，直到模型最终给出答案。
	•	为了实现ReAct，训练数据需要包含示例：问题->思考->动作->观测->…->最终答案。可以通过少样本提示教会模型这种格式，或者专门构造含工具使用的微调数据（如包含调用计算器、百科查询等步骤的对话）。
	•	工具调用能够有效弥补模型参数中知识的不足，提高事实准确性。例如，在涉及数学计算的问题上，借助计算器可以完全避免模型算错 ￼；在知识问答中，通过检索最新资料可以显著减少幻觉和过时信息。
	•	开源项目如Toolformer、LangChain Agent等提供了框架，使模型能够学习在生成中特定格式触发工具。LangChain尤其常用，它预先定义各类工具接口，模型按约定格式请求即可使用。微调时，我们可以加入对应的few-shot或者强化学习，使模型学会何时以及如何调用工具（例如当模型不确定某个事实时，就输出Search(...)动作）。

总之，工具增强让LLM具备了与外界交互的能力，扩展了纯语言模型的闭世界。我们的Pipeline可以考虑在微调末期加入带工具使用的数据（如要求模型算数用<<calc>>标签标记计算），或者训练完毕后在推理服务中对接工具接口，让模型通过特定格式输出实现工具使用。

(7) 检索增强生成（Retrieval-Augmented Generation, RAG）

除了动态调用工具，另一种增强模型知识和推理的方法是检索增强。RAG通过在生成答案前检索相关资料，将外部知识融入上下文。具体做法是：
	•	建立一个外部知识库或向量数据库（如包含维基百科文本的向量索引）。当有查询时，先用查询去检索最相关的文档片段，然后将这些内容附加在模型输入上。
	•	模型接收到的问题+检索结果作为完整上下文，再进行思维链推理和回答。因为有了相关背景，模型更容易进行正确推理，并减少杜撰。
	•	例如，对于需要事实支持的问题，模型通过RAG拿到一段维基百科条目，推理过程可以引用其中的信息，从而推理过程和答案都更加准确且有依据。
	•	检索增强可以与Chain-of-Thought结合：模型先根据问题想“我需要哪方面的信息”，然后用这串思考作为检索query获取资料，再继续推理。这类似于OpenAI WebGPT或少样本自引导检索的方法。

在Pipeline实现RAG，需要有检索组件（如FAISS、Milvus或Elasticsearch）和文本嵌入模型预先索引知识库。训练时可以用检索结果作为条件来微调模型，使其习惯参考外部文本。例如，提供格式：“查询：{问题}\n文档：{检索到的文本}\n思考与回答：…”。RAG的目标是以查询增强推理，它对于知识密集型推理任务特别有效（既降低模型幻觉，又保证知识更新及时）。

综上，我们有多种推理增强方法，可在微调Pipeline中灵活组合。例如，可以先监督微调CoT，再用DPO融入人类偏好，之后RLHF进一步优化；推理时结合自洽采样+Verifier过滤，并借助工具和检索确保准确性。最佳方案取决于任务需求和资源，但总体理念是不仅让模型“学会答案”，更要“学会思考”，通过多种手段提升模型的推理可靠性和复杂问题求解能力。

4. 训练阶段设计

在完成数据和方法准备后，就进入模型微调训练阶段的实施。为了充分发挥上述各种技术，我们通常采用多阶段训练方案，并设计合适的损失函数、选择恰当的训练框架和工具来支持整个过程。
	•	多阶段训练流程：大模型微调往往不是一步完成，而是按阶段逐步逼近目标性能。一个常见的三阶段流程是：
	1.	监督微调 (SFT)：首先，在我们准备的有监督数据（包含思维链和答案）上进行微调，让模型具备基本的指令遵循和推理输出能力。这一步如前所述，对应CoT模仿学习。训练产出一个SFT模型。
	2.	奖励模型训练 (RM)：并行地，我们使用收集的人类偏好数据（或其他评分依据）训练一个奖励模型，用于评估模型输出的优劣。这一步在使用RLHF时需要。如果采用DPO/GRPO，可以跳过RM直接用偏好数据指导策略。
	3.	强化学习微调 (RL)：最后，使用上一阶段的奖励模型作为反馈，通过PPO或其他算法进一步微调SFT模型，使之产生更符合人类偏好的输出 ￼。对于DPO，则是在此阶段用偏好对直接优化模型，无需真正的RL循环。
此外，还有逐层解冻训练、先粗调后精调等手法：例如先用一个较高学习率在LoRA层训练几个epoch让模型快速适应新任务，再降低学习率长时间训练提升性能；或者SFT时先锁定embedding层，待模型稳定后再解冻更多参数训练等等。这些策略旨在稳定训练过程和防止灾难性遗忘。
	•	损失函数设计：不同阶段采用的损失各异：
	•	SFT阶段使用标准的交叉熵损失，让模型最大化正确答案（包括推理过程和最终答案）的似然。有时会对思维链部分和答案部分分别加权，确保模型既重视推理过程也给出正确答案。例如，对最终答案的loss乘以略大的权重以强化正确答案输出。
	•	RLHF阶段使用强化学习目标，一般是PPO的目标函数，其中包含模型输出获得的奖励值，以及与原始模型分布的KL散度惩罚项。这相当于最大化$\mathbb{E}[R] - \beta \mathrm{KL}( \pi || \pi_{\text{SFT}})$，其中$\beta$控制模型别偏离SFT模型太远。这个奖励$R$来自我们训练的RM，对人类偏好高的输出给高分。最终通过策略梯度调整模型参数以提升$R$。
	•	DPO阶段则直接使用偏好交叉熵损失：对于每对比较，定义$P(\text{preferred}|\theta) = \frac{\exp(s_{\theta}(x_{\text{pref}}))}{\exp(s_{\theta}(x_{\text{pref}}))+\exp(s_{\theta}(x_{\text{nonpref}}))}$，最大化这个概率的对数。这里$s_{\theta}$是模型对输出算的得分（可取为对数概率和）。这样优化相当于直接提升偏好输出的得分，本质上是一个简单的分类损失。
	•	若模型需要同时考虑多种目标（如既要正确又要解释充分），也可以采用多任务损失或加权和的损失函数。另外，一些研究尝试在RLHF中融入价值对齐的奖励（如安全、礼貌等的额外惩罚），这些都可以通过定制损失来实现。
	•	训练框架与基础设施：选择合适的框架能够大大简化实现难度并提高训练效率：
	•	使用🤗 Transformers库的Trainer或Accelerate可以方便地进行基本的SFT训练。配合PEFT库，可以在几行代码内应用LoRA/QLoRA等高效微调技术。
	•	针对RLHF，Hugging Face推出了**TRL (Transformer Reinforcement Learning)**库，其中的PPOTrainer封装了从SFT模型加载、调用奖励模型、执行PPO更新的一整套流程，用户只需提供自己的模型和奖励函数即可开始训练。TRL也开始支持DPO等新的偏好优化方法，使实现更加便捷。
	•	DeepSpeed和DeepSpeed-Chat：微软的DeepSpeed提供了对大模型训练的内存和速度优化方案（如ZeRO并行、CPU offload等）。DeepSpeed-Chat是其开源的RLHF训练流水线，实现了高效的SFT->RM->PPO流程，号称可比现有RLHF系统快15倍，显著降低大模型对齐训练的成本。如果训练的是数十亿参数级模型，DeepSpeed有助于分布式训练和优化显存占用。
	•	监控与日志：在长时间训练中，借助工具（如Weights & Biases, TensorBoard）监控loss、奖励分数、KL散度等曲线十分重要，以便及时发现训练发散或过拟合。同时保留定期的checkpoint（如每隔若干步保存一次模型）以防止中途失败或需要回滚调整超参。
	•	vLLM（Inference Optimizer）：虽然vLLM主要用于推理，但也可以在训练阶段用于加速验证集推理、提升吞吐。在有复杂生成评估时，vLLM的高并发支持有用武之地。
	•	数据并行和混合精度：若有多卡环境，可以使用Torch DDP或DeepSpeed的并行方案，以数据并行方式加速训练；并使用FP16或BF16混合精度来提升运算速度和减小显存占用（Transformer库和DeepSpeed均易于开启混合精度训练）。
	•	模型保存机制：微调完成后，我们需要保存模型供推理使用。根据采用的PEFT策略不同，保存的内容也不同：
	•	全参数微调则直接保存整个模型权重（可能包含几个GB至几十GB）。
	•	LoRA微调通常只保存LoRA插入的权重（很小的权重文件）以及基座模型的引用。下次使用时需将LoRA权重加载到相应基座模型上。也可以选择将LoRA权重合并到基模权重后保存一个完整模型（称为“LoRA合并”）。
	•	QLoRA微调需保存4-bit量化的模型以及LoRA权重，推理时确保使用相应的推理库来加载（如Transformers + bitsandbytes能够直接加载4-bit模型）。
	•	最终部署往往会将模型转换为更加高效的格式，如ONNX或者TorchScript。这需要一些后处理，但在训练阶段就应考虑可导出性，尽量避免使用训练时才有而推理时无法支持的算子。

训练阶段是整个Pipeline的核心，我们通过多阶段逐步优化模型：先让模型具备基本的推理能力，再用人类偏好对齐，让模型的回答既正确又符合用户期望。配合合理的技术栈和监控，我们可以高效地得到一个性能卓越且稳定的LLM。

5. 推理阶段设计(2days)

模型训练完成后，在实际应用中还需要精心设计推理（Inference）阶段的配置，以充分利用模型能力并确保输出质量。在推理时，我们可以调整解码策略，并结合验证、搜索、工具调用等机制，得到更好的答案。以下是推理阶段的几个重要方面：
	•	解码策略配置：大型语言模型的生成行为很大程度由解码超参数控制。常见设置包括：
	•	温度 (temperature)：控制随机性，高温度（如1.0以上）让输出更多样但可能出错，低温度（接近0）让输出更确定但可能死板。对于有确定答案的推理任务，一般用较低温度以提高可靠性；而若要用自洽采样方法，可适当提高温度（如0.7~0.8）以便生成不同思路 ￼。
	•	最大长度：需足够容纳完整推理过程和答案。可根据训练中思维链平均长度设定一个上限，避免模型输出过长的不相关内容。
	•	阻止词：为了格式一致性，可以设置停止生成的触发标记。例如当模型输出了“答案：”并跟随一个换行后，就停止继续生成，防止闲聊跑题。
	•	Top-k / Top-p：这些采样参数控制从概率分布选择词的范围。对于推理过程，一般使用较高的top-p（0.9~1.0）以鼓励多样性，但最终答案部分希望准确时可临时降低随机性。
	•	Beam Search：对一些确定性任务，可以采用Beam Search探索多个可能输出并选择最高得分的。Beam Search适合短文本输出，但对于长推理链会导致搜索空间爆炸，不太实用。而且Beam Search倾向于高概率序列，可能反而局限了思路。因此，大部分推理增强设置倾向于随机采样 + 多次生成而非beam搜索。
	•	自洽投票：这是上文提到的Self-Consistency策略。实现上，就是运行上述解码过程N次，收集N个结果，然后统计哪个最终答案出现次数最多。这种投票可以显著提高准确率，特别是在N较大时。不过N太大会增加延迟，一般在5~10次左右权衡效果和成本。
	•	验证器/评论者集成：如果先前训练了Verifier模型或采用了强模型评估，我们可以在推理时集成判断环节。典型方案：
	•	单次过滤：让模型先按照普通策略生成一个回答，然后把完整的“问题+推理+答案”送入Verifier，由Verifier判断是否正确或是否高质量。如果Verifier打分低，我们可以选择不直接输出该答案，而是让模型重试（例如改变随机种子再生成一次，或尝试另一个prompt策略）。这个过程相当于给模型一次自我检查的机会。如果连续几次仍未通过Verifier阈值，则可以考虑输出最好的一个并提示不确定性。
	•	多候选比选：让模型一次生成多个候选答案（可以用批量解码或循环采样得到），然后Verifier对每个打分，选择最高分的作为最终答案输出。这种方法要求Verifier有良好的区分能力。对明确对错的问题（如数学题），Verifier可以是精确的判题函数；对开放问答，Verifier可以是一个训练的质量评估模型或者采用LLM对比两个答案“哪个更符合问题”。
	•	人类在环：在关键应用中，我们甚至可以把Verifier阶段交给人类审阅——模型给出几个备选推理方案，由人选一个。这实际上形成了半自动的AI助手，但可极大提高结果可靠性。不过这种方式成本高、速度慢，仅在需要高精度且有人工介入的场景采用。
	•	多路径生成与选择：应用如**思维树 (ToT)**需要在推理阶段实现一个搜索算法。我们可以将其融入服务端逻辑：
	•	编写一个递归或迭代的推理函数：输入当前问题或子问题给模型，生成若干候选思维；根据模型的评估（可能以特殊标记的输出或者调用Verifier模型）筛选候选；对筛选通过的候选继续生成下一步，合并上下文重复，形成树状搜索。
	•	为了避免爆炸，可以设定搜索宽度和深度上限，比如每步最多展开3个想法，最多探索5步深。如果任务没解出就停止，并可能返回“未找到解”。
	•	在实现Tree-of-Thoughts时，可能需要设计一些提示模板来引导模型给每个候选一个估价。例如，对每个候选思路，让模型判断“这条路线通往正确答案的可能性：大/中/小”。根据这个判断来保留或者丢弃节点。
	•	虽然复杂，但这种多路径探索能显著提高复杂搜索类问题的解决率。如果不采用全自动的ToT算法，也可以退而求其次：让模型一次性输出多个不同推理路径（比如通过一个特殊prompt：“请给出两种不同的解法”），然后我们选择其中正确的输出。这其实是人工构造的多路径，适合一些需要查看多种方案的场合。
	•	工具与检索接口接入：在实际部署时，我们需要将训练时设计的工具使用能力连接到真实环境中：
	•	如果模型学会了输出类似[检索]篮球的直径是多少的指令，那么推理服务需要捕捉到[检索]这类token，一旦检测到就提取后续查询内容，调用真实的搜索API获取结果，再将结果插入模型后续输入。例如，可以约定模型在工具动作后会等待外部信息，我们则用特殊标记把结果返给模型。这类似实现一个简化版的Agent。
	•	对于计算器、数据库查询等其他工具，过程类似：根据模型动作调用对应函数，把结果转换成模型可读的文本。要确保模型知道结果格式，比如计算结果前后可能有标识，使模型意识到那是它请求的输出。
	•	检索增强（RAG）方面，如果采用了向量数据库，我们可以在每次用户提问时先用embedding搜索k篇相关文章，把它们插入到用户提问后面，再交给模型生成回答。这部分可以通过LangChain的RetrievalQA链来实现，它会自动将检索内容与prompt拼接。
	•	此外，还有记忆管理：在多轮对话中，我们可以把过往对话内容摘要存入向量库，需要时检索重要信息放入context，从而实现长程对话记忆。这也是推理阶段可能用到的技巧。
工具与检索的接入需要在模型推理服务器中编程实现。幸运的是，已有不少开源方案可用，例如LangChain、LlamaIndex等，都提供了管理LLM与工具交互的模块。我们只需按照模型输出的格式协议，将环境接口连接好，即可让微调后的模型真正“用”起来这些增强能力。
	•	性能与并发：推理阶段还要考虑效率。使用经过优化的推理引擎（如vLLM或 FasterTransformer）可以加快生成速度、支持更高并发。如果模型部署在云端服务，通过批量请求、异步处理等手段也可提升吞吐量。配置合理的GPU资源来服务高峰请求（或者在低负载时自动降级到CPU）也属于部署阶段的优化内容，但这偏向工程实现，这里不展开。

总而言之，推理阶段的设计直接影响到用户最终体验。通过灵活调整生成策略和引入验证与工具，模型可以在不同时刻展现不同长处：即既保持严谨性，又不失创造力。完善的推理Pipeline应该在保证正确率的同时，尽可能发挥模型的推理潜能，让复杂问题得到可靠的解决。

6. 模型评估与部署(1weeks)

训练完一个推理增强的LLM后，最后的环节是对模型进行客观的评估，然后将其部署到实际应用中，并持续监控和改进。在这一阶段，我们关注模型性能指标、测试方法，以及如何将模型集成到应用环境并进行A/B测试和迭代。
	•	模型评估指标：针对推理型任务，我们需要多维度评估模型：
	•	准确率/正确率：最基本的指标，在有标准答案的任务上衡量模型答案正确的比例。例如在数学题或客观问答上计算正确率，在推理题上看最终结论对错。这直接反映模型实用性。
	•	思维链质量：评估模型给出的推理过程是否合逻辑且有用。这比较主观，可设计一些规则或人工评价标准。例如，看思维链中的每一步是否都正确推导，不犯常识或数学错误，以及推理是否严谨。对于提供了参考思维链的数据集，可以计算模型生成的思维链与参考的匹配程度，或看模型是否覆盖了解题所需的关键步骤。
	•	多样性：如果模型需要能给出不同风格的解释或方案，可以评估它在相同问题上多次回答的多样性，例如思维路径是否不同，还是每次都重复同样的句子。多样性高表示模型没有一味背答案，而是有“举一反三”的能力，但前提是正确率也要有保障。
	•	自洽性：自洽性指模型输出的前后是否一致、推理过程与最终答案是否自洽。如果模型思维链某步推导出X，但最后答案却给了不相关的Y，那就是不自洽。我们可以检查模型思维链中推导的最终中间结论是否等于它报出的答案，或在对话场景下评估模型的回答与之前内容是否矛盾。这种一致性在复杂推理中特别重要。
	•	鲁棒性：即模型对题面扰动、不同措辞是否仍能稳健作答。可以对问题做些小改动再测模型答案是否一致，以发现模型是否过度依赖表面模式而不是深层推理。
	•	效率：推理速度、计算消耗等也是衡量因素。特别在部署中，一个模型每秒能处理多少token关系到服务成本。我们在评估阶段可以记录平均响应时间等。
	•	此外还有安全性、真实性等通用评估维度。如果模型推理有可能编造不实信息，也需在事实性上评测；若部署面对用户，还要确保推理过程没有产生不当内容。
	•	开源评测框架：为了客观衡量模型，我们可以利用已有的评测基准和工具：
	•	LM Evaluation Harness（EleutherAI提供）：一个支持多任务自动评测的工具集。我们可以使用它在诸多标准数据集上测试模型，包括数学、常识问答、推理等数据集的表现。Harness会调用模型生成答案，与标准答案对比算分，比如准确率、F1等。使用这个工具可以比较我们模型与其他模型在同一任务上的差距。
	•	OpenCompass开放评测：上海AI实验室推出的OpenCompass是一个全面的大模型评测平台。它支持上百个数据集、涵盖五大能力维度（知识、理解、推理等）的评测，并提供可复现的评测框架。我们可以将模型接入OpenCompass，对例如GSM8K、StrategyQA等任务统一评测，看看模型在推理能力榜单上的排名。OpenCompass还有针对中文的评测套件，适合我们的模型如果需要在中文环境下测试。
	•	PromptBench / AlpacaEval：一些开源项目提供了对话模型的比较评测，通过LLM对比输出判优。比如AlpacaEval用GPT-4来评估两个模型回答谁更好。这可以用于对比我们的模型与某标杆模型在复杂问答上的表现，让GPT-4给出“更符合人类期望”的判决。
	•	学术基准：如果我们的模型针对于学术推理，比如数学证明或竞赛题，可以引入特别的数据集如MATH、ProofWiki、ARC挑战集等进行专项评测，分析模型长链推理能力的极限。
	•	主观评价：对于思维链质量等自动指标难以衡量的，可以组织小规模的人类评测，让评委阅读模型的推理过程和答案，对其逻辑性、清晰度、正确性进行打分。这种人为打分在研究中常见，也可配合如ACCURACY一起，全面了解模型优劣。
	•	部署架构：评估满意后，就可以考虑将模型集成到实际应用中了。现代部署通常采用微服务架构，将LLM封装成一个服务接口：
	•	模型服务封装：使用轻量级的Web框架（如FastAPI、Flask）包装模型推理功能，提供REST API或WebSocket接口。FastAPI非常适合这种IO密集型服务，可以异步处理请求，提高吞吐。此外，模型服务需要做好并发控制和队列，防止过载。如果用了vLLM作为推理引擎，我们可以直接利用其内置的HTTP服务功能，或者通过Python接口将vLLM集成到FastAPI中，实现高并发的流式输出。
	•	硬件部署：视模型大小，我们可能选择单卡GPU部署、多卡并行部署，或者使用弹性伸缩的云服务。对于7B~13B模型，单张高性能GPU（如A100）即可胜任；更大的模型需要多GPU或模型并行技术。如果用户请求量大，可以部署多个实例做负载均衡。
	•	检索系统：如果实现RAG，那么需要旁路部署一个向量数据库服务。常用方案如Faiss（嵌入在服务内部）或独立的Milvus/Elasticsearch服务。我们需要预先将知识库嵌入并存储，然后在每次请求时由模型服务调用向量数据库的API完成检索。检索结果可以缓存以提升效率。
	•	工具接口：将模型服务与外部API连接。例如，如果允许模型上网查资料，我们需要在服务里集成HTTP请求，但要注意安全（避免模型任意调用危险接口）。可以限定一组安全的API（天气、百科等）供模型使用。LangChain提供的工具集成可以直接调用，我们也可以手工实现一个工具路由，根据模型输出解析要用哪个API。
	•	日志与监控：部署后持续记录模型交互日志，包括输入、输出、用时、反馈等。这既用于监控服务健康，也为后续数据反馈回训练做准备。如果发现模型在某类问题上一再出错，可以收集这些实例，加入下一个训练周期改进模型。
	•	A/B测试：在上线阶段，为验证我们的新模型相对已有模型的改进效果，常采用A/B测试：
	•	将一部分用户请求路由给旧版本模型（A），一部分给新模型（B），对比两者的用户满意度、问题解决率等指标。这个拆流比例可以是固定的，例如各50%，也可以随着时间动态调整（比如观察到B表现更好就逐步扩大B的流量）。
	•	收集用户反馈：可以显式让用户对答案thumb up/down，或者根据用户后续行为（比如是否还要追问，是否弃用回答）来间接衡量。统计一段时间后，如果B模型的反馈明显优于A，就可以考虑全量替换。
	•	在没有大量真人用户的情况下，也可以采用离线A/B，即用一批预先准备的问题集，让两个模型作答，然后请人工或用GPT-4评估哪个更好。这在一定程度上模拟真实比较，常用来筛选模型版本。
	•	A/B测试也要关注是否有明显的偏差：比如新模型是否在某类用户上更好但另一些更差，或者是否引入某些新的错误（哪怕总体满意度提升，也需留意出现的新问题类型）。
	•	在线更新：模型部署后并非一成不变，可以根据使用情况进行在线更新迭代：
	•	持续学习：将用户与模型交互中产生的新数据（尤其是模型错误案例或用户不满的案例）汇总起来。定期对这些数据进行人工标注或筛选，形成新的训练样本，不断微调模型。这类似于“经验回放”，让模型从自身错误中学习。但要小心控制学习频率和步幅，避免快速在线学习导致模型失控（例如被用户恶意引导）。
	•	热点补充：对于模型不知道的新知识（如最新事实、流行语），可以通过增量微调或直接更新检索库来弥补。例如新出现的公共事件，可以增加一些描述该事件的知识文档供模型检索，或者微调模型在与该事件相关问答上的表现。
	•	模型版本管理：每次更新都作为一个新版本保留，如果出现问题能快速回滚。可以采用灰度发布：新版本先小流量试运行，观察是否有异常（比如回答质量下降、出现偏差），如果稳定再逐步扩大覆盖面。
	•	用户反馈机制：鼓励用户提交反馈，例如哪回答不对，哪里解释不清。这些反馈既可以用于定性改进模型，也可作为数据加入训练。对于大型在线系统，也可以用统计分析方法，从日志中提取模型常见失败模式，然后有针对性地改进。

最后，模型的评估和部署是一个循环迭代的过程。评估让我们知晓模型尚有哪些不足，部署和用户交互又会暴露新问题——我们再收集数据，回到数据准备和训练阶段修正模型。通过这种持续的打磨，我们的LLM推理增强Pipeline才能在实际应用中保持领先性能。总之，从数据到模型，从训练到部署，每一步的精心设计和当前先进方法的结合，能够帮助我们打造一个在工业级应用中可靠、在研究探索中强大的大语言模型推理方案。
